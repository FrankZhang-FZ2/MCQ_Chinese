data <- data[c(number<90),]
}
for(i in 1:5){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(MCQ_CN,1,percent)
data <- data[c(number<90),]
}
View(MCQ_CN)
dict <- read_excel("Data/CLS_dictionary.xlsx") #dictionary
data <- read_excel("Data/CLS_cleaned.xlsx") #data
MCQ_CN <- data[grep("MCQ_1",colnames(data)):grep("MCQ_30",colnames(data))]
names <- dict %>%
filter(type == "numeric") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.numeric)
rm(names)
for(i in 1:5){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(MCQ_CN,1,percent)
data <- data[c(number<90),]
}
data <- MCQ_CN[c(number<90),]
dict <- read_excel("Data/CLS_dictionary.xlsx") #dictionary
data <- read_excel("Data/CLS_cleaned.xlsx") #data
MCQ_CN <- data[grep("MCQ_1",colnames(data)):grep("MCQ_30",colnames(data))]
dict <- read_excel("Data/CLS_dictionary.xlsx") #dictionary
data <- read_excel("Data/CLS_cleaned.xlsx") #data
MCQ_CN <- data[grep("MCQ_1",colnames(data)):grep("MCQ_30",colnames(data))]
dict <- read_excel("Data/CLS_dictionary.xlsx") #dictionary
data <- read_excel("Data/CLS_cleaned.xlsx") #data
MCQ_CN <- data[grep("MCQ_1",colnames(data)):grep("MCQ_30",colnames(data))]
View(MCQ_CN)
MCQ_CN <- data[grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data))]
data[grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data))]
dict <- read_excel("Data/CLS_dictionary.xlsx") #dictionary
data <- read_excel("Data/CLS_cleaned.xlsx") #data
MCQ_CN <- data[grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data))]
grep("MCQ_1$",colnames(data))
grep("MCQ_30",colnames(data))
MCQ_CN <- data[,grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data))]
data[,grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data))]
MCQ_CN <- data[,c(grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data)))]
data[,c(grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data)))]
c(grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data)))
View(data)
MCQ_CN <- data[,c(grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data)))]
for(i in 1:5){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(MCQ_CN,1,percent)
data <- MCQ_CN[c(number<90),]
}
dict <- read_excel("Data/CLS_dictionary.xlsx") #dictionary
data <- read_excel("Data/CLS_cleaned.xlsx") #data
MCQ_CN <- data[,c(grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data)))]
names <- dict %>%
filter(type == "numeric") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.numeric)
rm(names)
for(i in 1:5){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(MCQ_CN,1,percent)
MCQ_CN <- MCQ_CN[c(number<90),]
}
library(summarytools)
library(tidyverse) #data wrangling
library(codebook) #codebook generation
library(future) #reliability
library(ufs) #reliability
library(GGally) #reliability
library(GPArotation) #reliability
library(rio) #reading in different file types
library(labelled) #labeling data
library(psych)
library(corrplot)
library("psych")
library(mirt)
library(eRm)
library(mice)
library(lavaan)
library(semPlot)
library(parameters)
library(broom)
library(likert)
library(sjPlot)
library(readxl)
dict <- read_excel("Data/CLS_dictionary.xlsx") #dictionary
data <- read_excel("Data/CLS_cleaned.xlsx") #data
MCQ_CN <- data[,c(grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data)))]
names <- dict %>%
filter(type == "numeric") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.numeric)
rm(names)
for(i in 1:5){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(MCQ_CN,1,percent)
MCQ_CN <- MCQ_CN[c(number<90),]
}
number
#function for checking percentage of missing data (unit=%)
percent_missing <- function(x){
sum(is.na((x))/length(x)*100)
}
missing_R <- apply(MCQ_CN,1,percent_missing)
table(missing_R)
## use other read functions as appropriate for file type
dict <- read_excel("Data/CLS_dictionary.xlsx") #dictionary
data <- read_excel("Data/CLS_cleaned.xlsx") #data
#prepare the data just for MCQ analysis
MCQ_CN <- data[,c(grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data)))]
## Variable types
names <- dict %>%
filter(type == "numeric") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.numeric)
rm(names)
#function for checking percentage of missing data (unit=%)
percent_missing <- function(x){
sum(is.na((x))/length(x)*100)
}
missing_R <- apply(MCQ_CN,1,percent_missing)
table(missing_R)
#check where the NAs are if needed
rindex <- rep(FALSE, nrow(MCQ_CN))
for (i in 1:nrow(MCQ_CN)){
for (j in 1:grep("MCQ_30",colnames(MCQ_CN))){
if( is.na(MCQ_CN[i,j])){
rindex[i] = TRUE
j = ncol(MCQ_CN)+1
}
}
}
data_error <- MCQ_CN[rindex,]
View(data_error)
rm(data_error)
table(missing_R)
#use if don't want imputation(turn off if I need imputation)
replace_rows <- subset(MCQ_CN, missing_R<=0)
replace_rows%>%rm
replace_rows%>%rm()
rm(replace_rows)
#use if don't want imputation(turn off if I need imputation)
MCQ_CN <- subset(MCQ_CN, missing_R<=0)
for(i in 1:5){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(MCQ_CN,1,percent)
MCQ_CN <- MCQ_CN[c(number<90),]
}
number
## assumption check
random_variable <- rchisq(nrow(MCQ_CN), 7)
fake_model <- lm(random_variable ~ .,
MCQ_CN = MCQ_CN[ , -c(86:91)])
fake_model <- lm(random_variable ~ .,
MCQ_CN)
standardized <- rstudent(fake_model)
fitvalues <- scale(fake_model$fitted.values)
plot(fake_model,2)#check for linearity
## assumption check
random_variable <- rchisq(nrow(MCQ_CN), 7)
fake_model <- lm(random_variable ~ .,
MCQ_CN)
standardized <- rstudent(fake_model)
fitvalues <- scale(fake_model$fitted.values)
plot(fake_model,2)#check for linearity
hist(standardized)#check for normality
{plot(standardized, fitvalues)
abline(v = 0)
abline(h = 0)
}#check for homogeneity + Homoscedasticity
standardized <- rstudent(fake_model)
fitvalues <- scale(fake_model$fitted.values)
plot(fake_model,2)#check for linearity
hist(standardized)#check for normality
?rstudent
## assumption check
random_variable <- rchisq(nrow(MCQ_CN), 10)
fake_model <- lm(random_variable ~ .,
MCQ_CN)
standardized <- rstudent(fake_model)
fitvalues <- scale(fake_model$fitted.values)
plot(fake_model,2)#check for linearity
hist(standardized)#check for normality
rchisq(nrow(MCQ_CN), 10)
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(tidyverse)
library(foreign)
library(lme4)
library(lmerTest)
library(mvnormtest)
library(mvnormalTest)
library(psych)
library(ggpubr)
library(rstatix)
library(heplots)
#MLM
MLM <- read_sav("MLM.sav")
#MANOVA
Homeless <- read_sav("Homelessness - revised.sav")
#one way ANOVA in regression
#turn Homeless variable into factor with 3 levels.
Homeless$Homeless <- factor(Homeless$Homeless,
level = c("1","2","3"),
label = c("chronic","acute","currently not homeless"))
#descriptive analysis
describeBy(QOL+GHS ~ Homeless, data = Homeless)
#change to numeric variables
Homeless[,-1] <- apply(Homeless[,-1], 2 , as.numeric)
Homeless %>%
ggplot()+
geom_density(aes(x=QOL, color = "Quality Of Life"))+
geom_density(aes(x=GHS, color = "General Health Status"))
#assumption about correlation
cor_matrix <-
Homeless %>%
select(!Homeless)%>%
cor() # check correlation between two variables
cor_matrix
cortest.bartlett(cor_matrix, n = nrow(Homeless))
?cortest.bartlett(cor_matrix, n = nrow(Homeless))
View(Homeless)
library(summarytools)
library(tidyverse) #data wrangling
library(codebook) #codebook generation
library(future) #reliability
library(ufs) #reliability
library(GGally) #reliability
library(GPArotation) #reliability
library(rio) #reading in different file types
library(labelled) #labeling data
library(psych)
library(corrplot)
library(psych)
library(mirt)
library(eRm)
library(mice)
library(lavaan)
library(semPlot)
library(parameters)
library(broom)
library(likert)
library(sjPlot)
library(readxl)
library(foreign)
library(lme4)
library(lmerTest)
library(mvnormtest)
library(mvnormalTest)
library(ggpubr)
library(rstatix)
library(heplots)
library(summarytools)
library(tidyverse) #data wrangling
library(codebook) #codebook generation
library(future) #reliability
library(ufs) #reliability
library(GGally) #reliability
library(GPArotation) #reliability
library(rio) #reading in different file types
library(labelled) #labeling data
library(psych)
library(corrplot)
library(psych)
library(mirt)
library(eRm)
library(mice)
library(lavaan)
library(semPlot)
library(parameters)
library(broom)
library(likert)
library(sjPlot)
library(readxl)
library(foreign)
library(lme4)
library(lmerTest)
library(mvnormtest)
library(mvnormalTest)
library(ggpubr)
library(rstatix)
library(heplots)
## use other read functions as appropriate for file type
dict <- read_excel("Data/CLS_dictionary.xlsx") #dictionary
data <- read_excel("Data/CLS_cleaned.xlsx") #data
#prepare the data just for MCQ analysis
MCQ_CN <- data[,c(grep("MCQ_1$",colnames(data)):grep("MCQ_30",colnames(data)))]
## Variable types
names <- dict %>%
filter(type == "numeric") %>%
pull(variable)
data[,names] <-
lapply(data[,names], as.numeric)
rm(names)
#function for checking percentage of missing data (unit=%)
percent_missing <- function(x){
sum(is.na((x))/length(x)*100)
}
missing_R <- apply(MCQ_CN,1,percent_missing)
table(missing_R)
#check where the NAs are if needed
rindex <- rep(FALSE, nrow(MCQ_CN))
for (i in 1:nrow(MCQ_CN)){
for (j in 1:grep("MCQ_30",colnames(MCQ_CN))){
if( is.na(MCQ_CN[i,j])){
rindex[i] = TRUE
j = ncol(MCQ_CN)+1
}
}
}
data_error <- MCQ_CN[rindex,]
rm(data_error)
#use if don't want imputation(turn off if I need imputation)
MCQ_CN <- subset(MCQ_CN, missing_R<=0)
##remove people that gave 90% of the same answer in MCQ
for(i in 1:5){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(MCQ_CN,1,percent)
MCQ_CN <- MCQ_CN[c(number<90)]
}
for(i in 1:5){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(MCQ_CN,1,percent)
MCQ_CN <- MCQ_CN[c(number<90)]
}
MCQ_CN <- MCQ_CN[c(number<90),]
#function for checking percentage of missing data (unit=%)
percent_missing <- function(x){
sum(is.na((x))/length(x)*100)
}
missing_R <- apply(MCQ_CN,1,percent_missing)
table(missing_R)
#check where the NAs are if needed
rindex <- rep(FALSE, nrow(MCQ_CN))
for (i in 1:nrow(MCQ_CN)){
for (j in 1:grep("MCQ_30",colnames(MCQ_CN))){
if( is.na(MCQ_CN[i,j])){
rindex[i] = TRUE
j = ncol(MCQ_CN)+1
}
}
}
data_error <- MCQ_CN[rindex,]
rm(data_error)
#use if don't want imputation(turn off if I need imputation)
MCQ_CN <- subset(MCQ_CN, missing_R<=0)
##remove people that gave 90% of the same answer in MCQ
for(i in 1:5){
percent <- function(x){
sum((x == i)/length(x)*100)
}
number <- apply(MCQ_CN,1,percent)
MCQ_CN <- MCQ_CN[c(number<90),]
}
#assumption about correlation
cor_matrix <-
MCQ_CN%>%
cor() # check correlation between two variables
cor_matrix
cortest.bartlett(cor_matrix, n = nrow(Homeless))
cortest.bartlett(cor_matrix, n = nrow(MCQ_CN))
#multivariate normality
mardia(MCQ_CN)
mvnTest(MCQ_CN)
mshapiro_test(MCQ_CN)
#Homogeneity
dvs <- MCQ_CN %>%
gather (key = "variable", value = "value") %>%
group_by(variable)
View(dvs)
View(dvs)
levene_test(value ~ MCQ_CN, data = dvs)
levene_test(value ~ variable, data = dvs)
## assumption check
random_variable <- rchisq(nrow(MCQ_CN), 10)
fake_model <- lm(random_variable ~ .,
MCQ_CN)
standardized <- rstudent(fake_model)
fitvalues <- scale(fake_model$fitted.values)
plot(fake_model,2)#check for linearity
plot(fake_model,1)#check for linearity
plot(fake_model,2)#check for linearity
View(fake_model)
## assumption check
random_variable <- rchisq(nrow(MCQ_CN), 5)
fake_model <- lm(random_variable ~ .,
MCQ_CN)
standardized <- rstudent(fake_model)
fitvalues <- scale(fake_model$fitted.values)
plot(fake_model,2)#check for linearity
corrplot(cor_matrix)
View(cor_matrix)
plot(fake_model,2)#check for linearity
hist(standardized)#check for normality
#multivariate normality
?mardia(MCQ_CN)
?mvnTest(MCQ_CN)
?mshapiro_test(MCQ_CN)
abline(v = 0)
{plot(standardized, fitvalues)
abline(v = 0)
abline(h = 0)
}#check for homogeneity + Homoscedasticity
View(MCQ_CN)
GM =~ MCQ_1 + MCQ_2 + MCQ_3 + MCQ_4 + MCQ_5 + MCQ_6
MCQ_CN_7M_model <- '
GM =~ MCQ_1 + MCQ_2 + MCQ_3 + MCQ_4 + MCQ_5 + MCQ_6
'
MCQ_CN_7M_fit <- cfa(
model = MCQ_CN_7M_model,
data = MCQ_CN,
std.lv = TRUE)
summary(MCQ_CN_7M_fit,
standardized = TRUE,
rsquare = TRUE,
fit.measures=TRUE)
parameterestimates(MCQ_CN_7M_fit,
standardized = TRUE)
fitmeasures(MCQ_CN_7M_fit)
semPaths(MCQ_CN_7M_fit,
whatLabels = "std",
what = "std",
layout = "tree2",
edge.label.cex = 1)
model_parameters(MCQ_CN_7M_fit, standardize = TRUE)
'
MCQ_CN_7M_model <- '
GM =~ MCQ_1 + MCQ_2 + MCQ_3 + MCQ_4 + MCQ_5 + MCQ_6,
H =~ MCQ_7 + MCQ_8 + MCQ_9 + MCQ_10,
C =~ MCQ_11 + MCQ_12 + MCQ_13 + MCQ_14,
F =~ MCQ_15 + MCQ_16 + MCQ_17 + MCQ_18,
L =~ MCQ_19 + MCQ_20 + MCQ_21 + MCQ_22,
P =~ MCQ_23 + MCQ_24 + MCQ_25 + MCQ_26,
R =~ MCQ_27 + MCQ_28 + MCQ_29 + MCQ_30
'
MCQ_CN_7M_model <- '
GM =~ MCQ_1 + MCQ_2 + MCQ_3 + MCQ_4 + MCQ_5 + MCQ_6,
'}
'}
{MCQ_CN_7M_model <- '
GM =~ MCQ_1 + MCQ_2 + MCQ_3 + MCQ_4 + MCQ_5 + MCQ_6
H =~ MCQ_7 + MCQ_8 + MCQ_9 + MCQ_10
C =~ MCQ_11 + MCQ_12 + MCQ_13 + MCQ_14
F =~ MCQ_15 + MCQ_16 + MCQ_17 + MCQ_18
L =~ MCQ_19 + MCQ_20 + MCQ_21 + MCQ_22
P =~ MCQ_23 + MCQ_24 + MCQ_25 + MCQ_26
R =~ MCQ_27 + MCQ_28 + MCQ_29 + MCQ_30
'}
View(MCQ_CN_7M_fit)
MCQ_CN_7M_fit <- cfa(
model = MCQ_CN_7M_model,
data = MCQ_CN,
std.lv = TRUE)
'}
{MCQ_CN_7M_model <- '
GM =~ MCQ_1 + MCQ_2 + MCQ_3 + MCQ_4 + MCQ_5 + MCQ_6
H =~ MCQ_7 + MCQ_8 + MCQ_9 + MCQ_10
'}
MCQ_CN_7M_fit <- cfa(
model = MCQ_CN_7M_model,
data = MCQ_CN,
std.lv = TRUE)
summary(MCQ_CN_7M_fit,
standardized = TRUE,
rsquare = TRUE,
fit.measures=TRUE)
parameterestimates(MCQ_CN_7M_fit,
standardized = TRUE)
summary(MCQ_CN_7M_fit,
standardized = TRUE,
rsquare = TRUE,
fit.measures=TRUE)
)
}
parameterestimates(MCQ_CN_7M_fit,
standardized = TRUE)
}}
stop
}
fitmeasures(MCQ_CN_7M_fit)
modificationindices(MCQ_CN_7M_fit,sort = T)
semPaths(MCQ_CN_7M_fit,
whatLabels = "std",
what = "std",
layout = "tree2",
edge.label.cex = 1)
model_parameters(MCQ_CN_7M_fit, standardize = TRUE)
library(likert)
library(sjPlot)
library(readxl)
library(foreign)
